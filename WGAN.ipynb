{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb01b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50f6967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Parameters\n",
    "learning_rate = 0.0002\n",
    "size_of_batch = 64\n",
    "num_epochs = 10\n",
    "critic_iter = 5\n",
    "noise_dim = 100\n",
    "output_folder=\"./WGANOutput\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3d446e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CIFAR10 Dataset\n",
    "data_set = datasets.CIFAR10(root=\"./data\", download=False, transform=transforms.Compose([\n",
    "    transforms.Resize(64),transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]))\n",
    "data_loader = torch.utils.data.DataLoader(data_set, batch_size = 128, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "569b2db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the weights for generator and discriminator\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "676ce5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Discriminator Class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.main=nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, 4, 1, 0),\n",
    "        )\n",
    "    def forward(self, passedInput):\n",
    "        return self.main(passedInput).mean(0).view(1)\n",
    "\n",
    "# Create Generator Class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        self.main=nn.Sequential(\n",
    "            nn.ConvTranspose2d(noise_dim, 512, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, passedInput):\n",
    "        return self.main(passedInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "965c501c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (main): Sequential(\n",
       "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (13): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Generator object\n",
    "discriminator = Discriminator()\n",
    "# Create Discriminator object\n",
    "generator = Generator()\n",
    "#apply initial weights\n",
    "discriminator.apply(weights_init)\n",
    "generator.apply(weights_init)\n",
    "discriminator\n",
    "generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7336fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "dis_optim = optim.RMSprop(discriminator.parameters(),lr=learning_rate)\n",
    "gen_optim = optim.RMSprop(generator.parameters(),lr=learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Initialize the parameters for training\n",
    "real_value = 1\n",
    "fake_value = 0\n",
    "input_tensor = torch.FloatTensor(size_of_batch, 3, 64, 64)\n",
    "noise = torch.FloatTensor(size_of_batch, noise_dim, 1, 1)\n",
    "norm_noise = torch.FloatTensor(size_of_batch, noise_dim, 1, 1).normal_(0, 1)\n",
    "norm_noise = Variable(norm_noise)\n",
    "ones_tensor = torch.FloatTensor([1])\n",
    "nOnesTensor=ones_tensor * (-1)\n",
    "criterion.cuda()\n",
    "\n",
    "\n",
    "# arrays for storing training losses and tracking progress\n",
    "disc_loss_list = []\n",
    "gen_loss_list = []\n",
    "count_list = []\n",
    "count = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f26302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10][5/391] DLoss: -0.5730 GLoss: 0.4709\n",
      "[0/10][10/391] DLoss: -0.4950 GLoss: 0.4682\n",
      "[0/10][15/391] DLoss: -1.0017 GLoss: 0.5958\n",
      "[0/10][20/391] DLoss: -1.1280 GLoss: 0.6319\n",
      "[0/10][25/391] DLoss: -1.1267 GLoss: 0.5834\n",
      "[0/10][30/391] DLoss: -0.7710 GLoss: 0.5792\n",
      "[0/10][35/391] DLoss: -0.9155 GLoss: 0.5967\n",
      "[0/10][40/391] DLoss: -1.0249 GLoss: 0.5649\n",
      "[0/10][45/391] DLoss: -1.0620 GLoss: 0.5288\n",
      "[0/10][50/391] DLoss: -0.7739 GLoss: 0.5962\n",
      "[0/10][55/391] DLoss: -1.1873 GLoss: 0.6410\n",
      "[0/10][60/391] DLoss: -1.0354 GLoss: 0.6182\n",
      "[0/10][65/391] DLoss: -1.1695 GLoss: 0.6659\n",
      "[0/10][70/391] DLoss: -1.3196 GLoss: 0.6961\n",
      "[0/10][75/391] DLoss: -0.9763 GLoss: 0.6706\n",
      "[0/10][80/391] DLoss: -1.2787 GLoss: 0.7075\n",
      "[0/10][85/391] DLoss: -1.3716 GLoss: 0.7159\n",
      "[0/10][90/391] DLoss: -1.3865 GLoss: 0.7094\n",
      "[0/10][95/391] DLoss: -1.3966 GLoss: 0.7297\n",
      "[0/10][100/391] DLoss: -1.4127 GLoss: 0.7326\n",
      "[0/10][105/391] DLoss: -1.4085 GLoss: 0.7345\n",
      "[0/10][110/391] DLoss: -1.4023 GLoss: 0.7213\n",
      "[0/10][115/391] DLoss: -1.2061 GLoss: 0.6505\n",
      "[0/10][120/391] DLoss: -1.2271 GLoss: 0.6682\n",
      "[0/10][125/391] DLoss: -1.2755 GLoss: 0.6574\n",
      "[0/10][130/391] DLoss: -1.0501 GLoss: 0.7033\n",
      "[0/10][135/391] DLoss: -1.2807 GLoss: 0.6841\n",
      "[0/10][140/391] DLoss: -1.2828 GLoss: 0.6896\n",
      "[0/10][145/391] DLoss: -1.0383 GLoss: 0.6642\n",
      "[0/10][150/391] DLoss: -1.1394 GLoss: 0.6858\n",
      "[0/10][155/391] DLoss: -0.9102 GLoss: 0.6415\n",
      "[0/10][160/391] DLoss: -0.8143 GLoss: 0.3075\n",
      "[0/10][165/391] DLoss: -0.9266 GLoss: 0.5936\n",
      "[0/10][170/391] DLoss: -1.1785 GLoss: 0.6311\n",
      "[0/10][175/391] DLoss: -1.0545 GLoss: 0.5678\n",
      "[0/10][180/391] DLoss: -0.3586 GLoss: 0.3860\n",
      "[0/10][185/391] DLoss: -1.2383 GLoss: 0.5458\n",
      "[0/10][190/391] DLoss: -1.1758 GLoss: 0.6697\n",
      "[0/10][195/391] DLoss: -1.0846 GLoss: 0.4966\n",
      "[0/10][200/391] DLoss: -0.4068 GLoss: 0.5608\n",
      "[0/10][205/391] DLoss: -0.8761 GLoss: 0.2396\n",
      "[0/10][210/391] DLoss: -0.8906 GLoss: 0.6225\n",
      "[0/10][215/391] DLoss: -0.6823 GLoss: 0.3215\n",
      "[0/10][220/391] DLoss: -0.6622 GLoss: 0.4684\n",
      "[0/10][225/391] DLoss: -1.0495 GLoss: 0.6507\n",
      "[0/10][230/391] DLoss: -1.3474 GLoss: 0.6906\n",
      "[0/10][235/391] DLoss: -1.3593 GLoss: 0.6775\n",
      "[0/10][240/391] DLoss: -1.3666 GLoss: 0.7116\n",
      "[0/10][245/391] DLoss: -1.3791 GLoss: 0.7210\n",
      "[0/10][250/391] DLoss: -1.4018 GLoss: 0.7102\n",
      "[0/10][255/391] DLoss: -1.3279 GLoss: 0.6238\n",
      "[0/10][260/391] DLoss: -0.9319 GLoss: 0.6850\n",
      "[0/10][265/391] DLoss: -1.3478 GLoss: 0.6959\n",
      "[0/10][270/391] DLoss: -1.3029 GLoss: 0.6855\n",
      "[0/10][275/391] DLoss: -1.1247 GLoss: 0.6485\n",
      "[0/10][280/391] DLoss: -1.2812 GLoss: 0.6769\n",
      "[0/10][285/391] DLoss: -1.2737 GLoss: 0.6870\n",
      "[0/10][290/391] DLoss: -1.2080 GLoss: 0.5792\n",
      "[0/10][295/391] DLoss: -0.6417 GLoss: 0.4701\n",
      "[0/10][300/391] DLoss: -1.1010 GLoss: 0.6127\n",
      "[0/10][305/391] DLoss: -0.6025 GLoss: 0.5128\n",
      "[0/10][310/391] DLoss: -1.0798 GLoss: 0.6509\n",
      "[0/10][315/391] DLoss: -1.2973 GLoss: 0.6552\n",
      "[0/10][320/391] DLoss: -1.3089 GLoss: 0.6594\n",
      "[0/10][325/391] DLoss: -1.1913 GLoss: 0.6759\n",
      "[0/10][330/391] DLoss: -1.3300 GLoss: 0.6788\n",
      "[0/10][335/391] DLoss: -1.3277 GLoss: 0.6880\n",
      "[0/10][340/391] DLoss: -1.2432 GLoss: 0.6368\n",
      "[0/10][345/391] DLoss: -0.5019 GLoss: 0.6749\n",
      "[0/10][350/391] DLoss: -1.2941 GLoss: 0.6626\n",
      "[0/10][355/391] DLoss: -1.2714 GLoss: 0.6619\n",
      "[0/10][360/391] DLoss: -1.3034 GLoss: 0.6833\n",
      "[0/10][365/391] DLoss: -1.2951 GLoss: 0.6483\n",
      "[0/10][370/391] DLoss: -1.3214 GLoss: 0.6955\n",
      "[0/10][375/391] DLoss: -1.3199 GLoss: 0.6694\n",
      "[0/10][380/391] DLoss: -1.3791 GLoss: 0.7086\n",
      "[0/10][385/391] DLoss: -1.3192 GLoss: 0.7102\n",
      "[0/10][390/391] DLoss: -1.3810 GLoss: 0.7006\n",
      "[0/10][391/391] DLoss: -1.3223 GLoss: 0.6515\n",
      "[1/10][5/391] DLoss: -0.6828 GLoss: 0.3259\n",
      "[1/10][10/391] DLoss: -1.2025 GLoss: 0.5721\n",
      "[1/10][15/391] DLoss: -1.2540 GLoss: 0.6750\n",
      "[1/10][20/391] DLoss: -1.3974 GLoss: 0.7199\n",
      "[1/10][25/391] DLoss: -1.4148 GLoss: 0.7173\n",
      "[1/10][30/391] DLoss: -1.4089 GLoss: 0.7170\n",
      "[1/10][35/391] DLoss: -1.3914 GLoss: 0.7111\n",
      "[1/10][40/391] DLoss: -1.2854 GLoss: 0.7024\n",
      "[1/10][45/391] DLoss: -1.3740 GLoss: 0.7078\n",
      "[1/10][50/391] DLoss: -0.9108 GLoss: 0.6646\n",
      "[1/10][55/391] DLoss: -1.2167 GLoss: 0.6125\n",
      "[1/10][60/391] DLoss: -1.1724 GLoss: 0.6645\n",
      "[1/10][65/391] DLoss: -1.3217 GLoss: 0.6929\n",
      "[1/10][70/391] DLoss: -1.3274 GLoss: 0.6811\n",
      "[1/10][75/391] DLoss: -1.1888 GLoss: 0.7025\n",
      "[1/10][80/391] DLoss: -1.3082 GLoss: 0.6765\n",
      "[1/10][85/391] DLoss: -1.3610 GLoss: 0.6994\n",
      "[1/10][90/391] DLoss: -1.3447 GLoss: 0.6923\n",
      "[1/10][95/391] DLoss: -1.3158 GLoss: 0.6765\n",
      "[1/10][100/391] DLoss: -1.2974 GLoss: 0.6924\n",
      "[1/10][105/391] DLoss: -1.3388 GLoss: 0.6951\n",
      "[1/10][110/391] DLoss: -1.3250 GLoss: 0.6821\n",
      "[1/10][115/391] DLoss: -1.3312 GLoss: 0.6663\n",
      "[1/10][120/391] DLoss: -1.3424 GLoss: 0.6756\n",
      "[1/10][125/391] DLoss: -1.3211 GLoss: 0.6752\n",
      "[1/10][130/391] DLoss: -1.2033 GLoss: 0.6498\n",
      "[1/10][135/391] DLoss: -1.3015 GLoss: 0.6727\n",
      "[1/10][140/391] DLoss: -1.2419 GLoss: 0.6670\n",
      "[1/10][145/391] DLoss: -1.3451 GLoss: 0.6846\n",
      "[1/10][150/391] DLoss: -1.0996 GLoss: 0.6763\n",
      "[1/10][155/391] DLoss: -1.2653 GLoss: 0.6843\n",
      "[1/10][160/391] DLoss: -1.3345 GLoss: 0.6893\n",
      "[1/10][165/391] DLoss: -1.3259 GLoss: 0.6694\n",
      "[1/10][170/391] DLoss: -1.3258 GLoss: 0.6792\n",
      "[1/10][175/391] DLoss: -0.9975 GLoss: 0.6479\n",
      "[1/10][180/391] DLoss: -1.3145 GLoss: 0.6579\n",
      "[1/10][185/391] DLoss: -1.3295 GLoss: 0.6741\n",
      "[1/10][190/391] DLoss: -1.0369 GLoss: 0.6361\n",
      "[1/10][195/391] DLoss: -1.1512 GLoss: 0.6181\n",
      "[1/10][200/391] DLoss: -1.3198 GLoss: 0.6830\n",
      "[1/10][205/391] DLoss: -1.3069 GLoss: 0.6533\n",
      "[1/10][210/391] DLoss: -1.2208 GLoss: 0.6936\n",
      "[1/10][215/391] DLoss: -1.3433 GLoss: 0.6861\n",
      "[1/10][220/391] DLoss: -1.3140 GLoss: 0.6800\n",
      "[1/10][225/391] DLoss: -1.2894 GLoss: 0.6589\n",
      "[1/10][230/391] DLoss: -1.2725 GLoss: 0.6656\n",
      "[1/10][235/391] DLoss: -1.2845 GLoss: 0.6856\n",
      "[1/10][240/391] DLoss: -1.2859 GLoss: 0.6461\n",
      "[1/10][245/391] DLoss: -1.0817 GLoss: 0.6724\n",
      "[1/10][250/391] DLoss: -1.3273 GLoss: 0.6769\n",
      "[1/10][255/391] DLoss: -1.3116 GLoss: 0.6851\n",
      "[1/10][260/391] DLoss: -1.2388 GLoss: 0.6347\n",
      "[1/10][265/391] DLoss: -1.0807 GLoss: 0.6553\n",
      "[1/10][270/391] DLoss: -1.2821 GLoss: 0.6587\n",
      "[1/10][275/391] DLoss: -1.3085 GLoss: 0.6803\n",
      "[1/10][280/391] DLoss: -1.1896 GLoss: 0.6931\n",
      "[1/10][285/391] DLoss: -1.2845 GLoss: 0.6569\n",
      "[1/10][290/391] DLoss: -1.2517 GLoss: 0.6575\n",
      "[1/10][295/391] DLoss: -1.2176 GLoss: 0.6477\n",
      "[1/10][300/391] DLoss: -1.3102 GLoss: 0.6661\n",
      "[1/10][305/391] DLoss: -0.6393 GLoss: 0.6508\n",
      "[1/10][310/391] DLoss: -1.2429 GLoss: 0.6645\n",
      "[1/10][315/391] DLoss: -1.3248 GLoss: 0.6834\n",
      "[1/10][320/391] DLoss: -1.2725 GLoss: 0.6480\n",
      "[1/10][325/391] DLoss: -1.2651 GLoss: 0.6933\n",
      "[1/10][330/391] DLoss: -1.2865 GLoss: 0.6653\n",
      "[1/10][335/391] DLoss: -1.2106 GLoss: 0.6713\n",
      "[1/10][340/391] DLoss: -1.2785 GLoss: 0.6581\n",
      "[1/10][345/391] DLoss: -0.9518 GLoss: 0.6582\n",
      "[1/10][350/391] DLoss: -1.2659 GLoss: 0.6432\n",
      "[1/10][355/391] DLoss: -1.2399 GLoss: 0.6629\n",
      "[1/10][360/391] DLoss: -1.2282 GLoss: 0.6444\n",
      "[1/10][365/391] DLoss: -1.2042 GLoss: 0.6534\n",
      "[1/10][370/391] DLoss: -1.2299 GLoss: 0.6615\n",
      "[1/10][375/391] DLoss: -1.2069 GLoss: 0.6708\n",
      "[1/10][380/391] DLoss: -1.1766 GLoss: 0.6142\n",
      "[1/10][385/391] DLoss: -0.9401 GLoss: 0.6359\n",
      "[1/10][390/391] DLoss: -1.2220 GLoss: 0.6427\n",
      "[1/10][391/391] DLoss: -0.9929 GLoss: 0.6405\n",
      "[2/10][5/391] DLoss: -0.9949 GLoss: 0.4760\n",
      "[2/10][10/391] DLoss: -1.2964 GLoss: 0.6634\n",
      "[2/10][15/391] DLoss: -1.2828 GLoss: 0.6535\n",
      "[2/10][20/391] DLoss: -1.1933 GLoss: 0.6670\n",
      "[2/10][25/391] DLoss: -1.1994 GLoss: 0.6575\n",
      "[2/10][30/391] DLoss: -1.0697 GLoss: 0.5032\n",
      "[2/10][35/391] DLoss: -0.7389 GLoss: 0.3571\n",
      "[2/10][40/391] DLoss: -1.0924 GLoss: 0.6013\n",
      "[2/10][45/391] DLoss: -1.0296 GLoss: 0.6667\n",
      "[2/10][50/391] DLoss: -1.2777 GLoss: 0.6530\n",
      "[2/10][55/391] DLoss: -1.2456 GLoss: 0.6594\n",
      "[2/10][60/391] DLoss: -1.1517 GLoss: 0.6646\n",
      "[2/10][65/391] DLoss: -1.1792 GLoss: 0.6105\n",
      "[2/10][70/391] DLoss: -1.0711 GLoss: 0.6740\n",
      "[2/10][75/391] DLoss: -1.1960 GLoss: 0.6425\n",
      "[2/10][80/391] DLoss: -1.2509 GLoss: 0.6197\n",
      "[2/10][85/391] DLoss: -1.2227 GLoss: 0.6710\n",
      "[2/10][90/391] DLoss: -1.2186 GLoss: 0.6503\n",
      "[2/10][95/391] DLoss: -0.9587 GLoss: 0.6319\n",
      "[2/10][100/391] DLoss: -0.9723 GLoss: 0.6072\n",
      "[2/10][105/391] DLoss: -1.2072 GLoss: 0.6355\n",
      "[2/10][110/391] DLoss: -1.2050 GLoss: 0.6046\n",
      "[2/10][115/391] DLoss: -1.2122 GLoss: 0.6386\n",
      "[2/10][120/391] DLoss: -1.2286 GLoss: 0.6440\n",
      "[2/10][125/391] DLoss: -1.1066 GLoss: 0.6649\n",
      "[2/10][130/391] DLoss: -1.1710 GLoss: 0.6191\n",
      "[2/10][135/391] DLoss: -1.2048 GLoss: 0.6815\n",
      "[2/10][140/391] DLoss: -1.2367 GLoss: 0.6255\n",
      "[2/10][145/391] DLoss: -1.2139 GLoss: 0.6670\n",
      "[2/10][150/391] DLoss: -1.2264 GLoss: 0.6169\n",
      "[2/10][155/391] DLoss: -1.1410 GLoss: 0.6861\n",
      "[2/10][160/391] DLoss: -1.2632 GLoss: 0.6334\n",
      "[2/10][165/391] DLoss: -1.2102 GLoss: 0.6548\n",
      "[2/10][170/391] DLoss: -1.2667 GLoss: 0.6239\n",
      "[2/10][175/391] DLoss: -1.0358 GLoss: 0.6814\n",
      "[2/10][180/391] DLoss: -1.1721 GLoss: 0.6107\n",
      "[2/10][185/391] DLoss: -1.1783 GLoss: 0.6205\n",
      "[2/10][190/391] DLoss: -1.1609 GLoss: 0.6392\n",
      "[2/10][195/391] DLoss: -1.1916 GLoss: 0.6183\n",
      "[2/10][200/391] DLoss: -1.1893 GLoss: 0.6489\n",
      "[2/10][205/391] DLoss: -1.2251 GLoss: 0.6030\n",
      "[2/10][210/391] DLoss: -1.1386 GLoss: 0.5981\n",
      "[2/10][215/391] DLoss: -1.0947 GLoss: 0.5953\n",
      "[2/10][220/391] DLoss: -1.1925 GLoss: 0.6472\n",
      "[2/10][225/391] DLoss: -1.1952 GLoss: 0.6156\n",
      "[2/10][230/391] DLoss: -1.2099 GLoss: 0.6369\n",
      "[2/10][235/391] DLoss: -1.2229 GLoss: 0.6154\n",
      "[2/10][240/391] DLoss: -1.2004 GLoss: 0.6505\n",
      "[2/10][245/391] DLoss: -1.2301 GLoss: 0.6443\n",
      "[2/10][250/391] DLoss: -1.1846 GLoss: 0.6468\n",
      "[2/10][255/391] DLoss: -1.1490 GLoss: 0.6203\n",
      "[2/10][260/391] DLoss: -0.5415 GLoss: 0.6537\n",
      "[2/10][265/391] DLoss: -1.1393 GLoss: 0.5591\n",
      "[2/10][270/391] DLoss: -1.1533 GLoss: 0.6634\n",
      "[2/10][275/391] DLoss: -1.1696 GLoss: 0.6145\n",
      "[2/10][280/391] DLoss: -1.0102 GLoss: 0.6799\n",
      "[2/10][285/391] DLoss: -1.2071 GLoss: 0.6002\n",
      "[2/10][290/391] DLoss: -1.2357 GLoss: 0.6651\n",
      "[2/10][295/391] DLoss: -1.2048 GLoss: 0.6171\n",
      "[2/10][300/391] DLoss: -1.1171 GLoss: 0.6490\n",
      "[2/10][305/391] DLoss: -1.1584 GLoss: 0.5589\n",
      "[2/10][310/391] DLoss: -1.0274 GLoss: 0.6684\n",
      "[2/10][315/391] DLoss: -1.2210 GLoss: 0.6351\n",
      "[2/10][320/391] DLoss: -1.1502 GLoss: 0.6688\n",
      "[2/10][325/391] DLoss: -1.1784 GLoss: 0.5703\n",
      "[2/10][330/391] DLoss: -0.8876 GLoss: 0.6661\n",
      "[2/10][335/391] DLoss: -1.1946 GLoss: 0.5931\n",
      "[2/10][340/391] DLoss: -1.0435 GLoss: 0.6267\n",
      "[2/10][345/391] DLoss: -0.9537 GLoss: 0.5799\n",
      "[2/10][350/391] DLoss: -1.1304 GLoss: 0.6135\n",
      "[2/10][355/391] DLoss: -1.1285 GLoss: 0.6758\n",
      "[2/10][360/391] DLoss: -1.1781 GLoss: 0.5604\n",
      "[2/10][365/391] DLoss: -1.0622 GLoss: 0.6121\n",
      "[2/10][370/391] DLoss: -1.0673 GLoss: 0.6524\n",
      "[2/10][375/391] DLoss: -1.1609 GLoss: 0.6008\n",
      "[2/10][380/391] DLoss: -0.8360 GLoss: 0.6508\n",
      "[2/10][385/391] DLoss: -1.0237 GLoss: 0.5614\n",
      "[2/10][390/391] DLoss: -1.1105 GLoss: 0.5727\n",
      "[2/10][391/391] DLoss: -0.5603 GLoss: 0.6494\n",
      "[3/10][5/391] DLoss: -1.0355 GLoss: 0.5784\n",
      "[3/10][10/391] DLoss: -1.1479 GLoss: 0.6544\n",
      "[3/10][15/391] DLoss: -1.1459 GLoss: 0.6020\n",
      "[3/10][20/391] DLoss: -1.0537 GLoss: 0.6132\n",
      "[3/10][25/391] DLoss: -0.7491 GLoss: 0.6621\n",
      "[3/10][30/391] DLoss: -1.1108 GLoss: 0.5678\n",
      "[3/10][35/391] DLoss: -1.1214 GLoss: 0.5733\n",
      "[3/10][40/391] DLoss: -1.1391 GLoss: 0.6005\n",
      "[3/10][45/391] DLoss: -1.1253 GLoss: 0.6504\n",
      "[3/10][50/391] DLoss: -1.1107 GLoss: 0.6051\n",
      "[3/10][55/391] DLoss: -0.9159 GLoss: 0.6581\n",
      "[3/10][60/391] DLoss: -1.0127 GLoss: 0.5200\n",
      "[3/10][65/391] DLoss: -1.0208 GLoss: 0.5121\n",
      "[3/10][70/391] DLoss: -1.1111 GLoss: 0.6014\n",
      "[3/10][75/391] DLoss: -1.0996 GLoss: 0.6271\n",
      "[3/10][80/391] DLoss: -1.1304 GLoss: 0.5832\n",
      "[3/10][85/391] DLoss: -0.9276 GLoss: 0.6678\n",
      "[3/10][90/391] DLoss: -1.2177 GLoss: 0.6389\n",
      "[3/10][95/391] DLoss: -1.1200 GLoss: 0.5793\n",
      "[3/10][100/391] DLoss: -0.9644 GLoss: 0.6298\n",
      "[3/10][105/391] DLoss: -1.1169 GLoss: 0.5708\n",
      "[3/10][110/391] DLoss: -0.9517 GLoss: 0.6644\n",
      "[3/10][115/391] DLoss: -1.1807 GLoss: 0.5880\n",
      "[3/10][120/391] DLoss: -0.9923 GLoss: 0.6383\n",
      "[3/10][125/391] DLoss: -1.0612 GLoss: 0.5609\n",
      "[3/10][130/391] DLoss: -1.0423 GLoss: 0.6447\n",
      "[3/10][135/391] DLoss: -1.1145 GLoss: 0.5642\n",
      "[3/10][140/391] DLoss: -0.8758 GLoss: 0.6418\n",
      "[3/10][145/391] DLoss: -1.1042 GLoss: 0.5603\n",
      "[3/10][150/391] DLoss: -0.9768 GLoss: 0.6391\n",
      "[3/10][155/391] DLoss: -1.0965 GLoss: 0.6010\n",
      "[3/10][160/391] DLoss: -1.0621 GLoss: 0.5968\n",
      "[3/10][165/391] DLoss: -1.0812 GLoss: 0.6458\n",
      "[3/10][170/391] DLoss: -1.0765 GLoss: 0.4813\n",
      "[3/10][175/391] DLoss: -0.7568 GLoss: 0.3815\n",
      "[3/10][180/391] DLoss: -1.0746 GLoss: 0.6486\n",
      "[3/10][185/391] DLoss: -1.1947 GLoss: 0.6251\n",
      "[3/10][190/391] DLoss: -1.1802 GLoss: 0.6326\n",
      "[3/10][195/391] DLoss: -0.9236 GLoss: 0.6284\n",
      "[3/10][200/391] DLoss: -0.6697 GLoss: 0.2121\n",
      "[3/10][205/391] DLoss: -0.0320 GLoss: -0.6144\n",
      "[3/10][210/391] DLoss: -0.1789 GLoss: -0.2708\n",
      "[3/10][215/391] DLoss: -1.1131 GLoss: 0.6262\n",
      "[3/10][220/391] DLoss: -1.2491 GLoss: 0.6490\n",
      "[3/10][225/391] DLoss: -1.3239 GLoss: 0.6856\n",
      "[3/10][230/391] DLoss: -1.3166 GLoss: 0.6783\n",
      "[3/10][235/391] DLoss: -1.3147 GLoss: 0.6806\n",
      "[3/10][240/391] DLoss: -1.2997 GLoss: 0.6543\n",
      "[3/10][245/391] DLoss: -1.1592 GLoss: 0.6514\n",
      "[3/10][250/391] DLoss: -1.1483 GLoss: 0.6401\n",
      "[3/10][255/391] DLoss: -0.9238 GLoss: 0.6467\n",
      "[3/10][260/391] DLoss: -0.8802 GLoss: 0.4054\n",
      "[3/10][265/391] DLoss: -1.0283 GLoss: 0.6168\n",
      "[3/10][270/391] DLoss: -1.1002 GLoss: 0.6171\n",
      "[3/10][275/391] DLoss: -1.1244 GLoss: 0.6338\n",
      "[3/10][280/391] DLoss: -0.9921 GLoss: 0.3422\n",
      "[3/10][285/391] DLoss: -0.7005 GLoss: 0.5293\n",
      "[3/10][290/391] DLoss: -1.1059 GLoss: 0.6425\n",
      "[3/10][295/391] DLoss: -1.1998 GLoss: 0.6048\n",
      "[3/10][300/391] DLoss: -1.1150 GLoss: 0.6493\n",
      "[3/10][305/391] DLoss: -1.1337 GLoss: 0.5712\n",
      "[3/10][310/391] DLoss: -1.0280 GLoss: 0.5026\n",
      "[3/10][315/391] DLoss: -0.9898 GLoss: 0.6338\n",
      "[3/10][320/391] DLoss: -1.1496 GLoss: 0.6224\n",
      "[3/10][325/391] DLoss: -1.0307 GLoss: 0.6745\n",
      "[3/10][330/391] DLoss: -0.8966 GLoss: 0.3063\n",
      "[3/10][335/391] DLoss: -0.2905 GLoss: -0.1442\n",
      "[3/10][340/391] DLoss: -0.9805 GLoss: 0.5000\n",
      "[3/10][345/391] DLoss: -1.1678 GLoss: 0.6281\n",
      "[3/10][350/391] DLoss: -1.1960 GLoss: 0.6523\n",
      "[3/10][355/391] DLoss: -1.1951 GLoss: 0.6187\n",
      "[3/10][360/391] DLoss: -0.9241 GLoss: 0.6736\n",
      "[3/10][365/391] DLoss: -1.2162 GLoss: 0.6344\n",
      "[3/10][370/391] DLoss: -1.0375 GLoss: 0.6388\n",
      "[3/10][375/391] DLoss: -1.0998 GLoss: 0.5544\n",
      "[3/10][380/391] DLoss: -0.8256 GLoss: 0.6399\n",
      "[3/10][385/391] DLoss: -1.0370 GLoss: 0.4662\n",
      "[3/10][390/391] DLoss: -0.9604 GLoss: 0.6322\n",
      "[3/10][391/391] DLoss: -0.9179 GLoss: 0.4556\n",
      "[4/10][5/391] DLoss: -0.9727 GLoss: 0.4941\n",
      "[4/10][10/391] DLoss: -1.0410 GLoss: 0.6586\n",
      "[4/10][15/391] DLoss: -1.1073 GLoss: 0.6096\n",
      "[4/10][20/391] DLoss: -1.1067 GLoss: 0.6319\n",
      "[4/10][25/391] DLoss: -1.0869 GLoss: 0.5644\n",
      "[4/10][30/391] DLoss: -0.8608 GLoss: 0.6474\n",
      "[4/10][35/391] DLoss: -0.9482 GLoss: 0.4877\n",
      "[4/10][40/391] DLoss: -0.9943 GLoss: 0.5201\n",
      "[4/10][45/391] DLoss: -0.9201 GLoss: 0.6419\n",
      "[4/10][50/391] DLoss: -1.0767 GLoss: 0.5412\n",
      "[4/10][55/391] DLoss: -0.9542 GLoss: 0.6438\n",
      "[4/10][60/391] DLoss: -1.1110 GLoss: 0.5774\n",
      "[4/10][65/391] DLoss: -1.1032 GLoss: 0.5600\n",
      "[4/10][70/391] DLoss: -1.0949 GLoss: 0.6171\n",
      "[4/10][75/391] DLoss: -1.1346 GLoss: 0.5759\n",
      "[4/10][80/391] DLoss: -0.9455 GLoss: 0.6342\n",
      "[4/10][85/391] DLoss: -0.8981 GLoss: 0.2316\n",
      "[4/10][90/391] DLoss: -1.0674 GLoss: 0.6076\n",
      "[4/10][95/391] DLoss: -1.1744 GLoss: 0.6392\n",
      "[4/10][100/391] DLoss: -1.2204 GLoss: 0.6244\n",
      "[4/10][105/391] DLoss: -1.2077 GLoss: 0.6469\n",
      "[4/10][110/391] DLoss: -1.1483 GLoss: 0.6197\n",
      "[4/10][115/391] DLoss: -1.0896 GLoss: 0.6674\n",
      "[4/10][120/391] DLoss: -0.5951 GLoss: 0.5120\n",
      "[4/10][125/391] DLoss: -0.9069 GLoss: 0.6367\n",
      "[4/10][130/391] DLoss: -0.8550 GLoss: 0.4017\n",
      "[4/10][135/391] DLoss: -0.9876 GLoss: 0.4715\n",
      "[4/10][140/391] DLoss: -1.1345 GLoss: 0.5885\n",
      "[4/10][145/391] DLoss: -1.1093 GLoss: 0.5980\n",
      "[4/10][150/391] DLoss: -0.9927 GLoss: 0.6624\n",
      "[4/10][155/391] DLoss: -0.9849 GLoss: 0.5212\n",
      "[4/10][160/391] DLoss: -0.9282 GLoss: 0.6434\n",
      "[4/10][165/391] DLoss: -1.0955 GLoss: 0.5890\n",
      "[4/10][170/391] DLoss: -1.1139 GLoss: 0.5588\n",
      "[4/10][175/391] DLoss: -1.0800 GLoss: 0.5778\n",
      "[4/10][180/391] DLoss: -1.1000 GLoss: 0.6166\n",
      "[4/10][185/391] DLoss: -1.0928 GLoss: 0.5370\n",
      "[4/10][190/391] DLoss: -1.0976 GLoss: 0.5766\n",
      "[4/10][195/391] DLoss: -1.0450 GLoss: 0.6589\n",
      "[4/10][200/391] DLoss: -0.7627 GLoss: 0.3511\n",
      "[4/10][205/391] DLoss: -1.1581 GLoss: 0.6262\n",
      "[4/10][210/391] DLoss: -1.2179 GLoss: 0.6338\n",
      "[4/10][215/391] DLoss: -1.1998 GLoss: 0.6297\n",
      "[4/10][220/391] DLoss: -1.1923 GLoss: 0.6167\n",
      "[4/10][225/391] DLoss: -1.1054 GLoss: 0.6378\n",
      "[4/10][230/391] DLoss: -1.1287 GLoss: 0.5210\n",
      "[4/10][235/391] DLoss: -0.9747 GLoss: 0.6657\n",
      "[4/10][240/391] DLoss: -0.7460 GLoss: 0.3793\n",
      "[4/10][245/391] DLoss: -0.9761 GLoss: 0.3767\n",
      "[4/10][250/391] DLoss: -0.7795 GLoss: 0.5616\n",
      "[4/10][255/391] DLoss: -1.2004 GLoss: 0.6408\n",
      "[4/10][260/391] DLoss: -1.2098 GLoss: 0.6286\n",
      "[4/10][265/391] DLoss: -1.0421 GLoss: 0.6725\n",
      "[4/10][270/391] DLoss: -1.1479 GLoss: 0.5796\n",
      "[4/10][275/391] DLoss: -0.9472 GLoss: 0.6363\n",
      "[4/10][280/391] DLoss: -0.8506 GLoss: 0.3332\n",
      "[4/10][285/391] DLoss: -1.0000 GLoss: 0.6360\n",
      "[4/10][290/391] DLoss: -1.1541 GLoss: 0.5975\n",
      "[4/10][295/391] DLoss: -1.1942 GLoss: 0.6105\n",
      "[4/10][300/391] DLoss: -1.1491 GLoss: 0.6169\n",
      "[4/10][305/391] DLoss: -0.9795 GLoss: 0.6696\n",
      "[4/10][310/391] DLoss: -0.7346 GLoss: 0.3488\n",
      "[4/10][315/391] DLoss: -1.0072 GLoss: 0.5320\n",
      "[4/10][320/391] DLoss: -1.1132 GLoss: 0.6584\n",
      "[4/10][325/391] DLoss: -1.1600 GLoss: 0.6001\n",
      "[4/10][330/391] DLoss: -1.1526 GLoss: 0.5470\n",
      "[4/10][335/391] DLoss: -0.9745 GLoss: 0.6688\n",
      "[4/10][340/391] DLoss: -1.0154 GLoss: 0.3727\n",
      "[4/10][345/391] DLoss: -0.7574 GLoss: 0.6421\n",
      "[4/10][350/391] DLoss: -1.1817 GLoss: 0.6154\n",
      "[4/10][355/391] DLoss: -1.0963 GLoss: 0.5988\n",
      "[4/10][360/391] DLoss: -0.8118 GLoss: 0.6498\n",
      "[4/10][365/391] DLoss: -1.1376 GLoss: 0.5957\n",
      "[4/10][370/391] DLoss: -1.1023 GLoss: 0.6055\n",
      "[4/10][375/391] DLoss: -1.0647 GLoss: 0.5385\n",
      "[4/10][380/391] DLoss: -0.8475 GLoss: 0.6559\n",
      "[4/10][385/391] DLoss: -1.0683 GLoss: 0.5042\n",
      "[4/10][390/391] DLoss: -0.9710 GLoss: 0.6383\n",
      "[4/10][391/391] DLoss: -0.9870 GLoss: 0.4596\n",
      "[5/10][5/391] DLoss: -1.0709 GLoss: 0.4887\n",
      "[5/10][10/391] DLoss: -1.0496 GLoss: 0.5896\n",
      "[5/10][15/391] DLoss: -1.1422 GLoss: 0.6056\n",
      "[5/10][20/391] DLoss: -0.8840 GLoss: 0.6586\n",
      "[5/10][25/391] DLoss: -0.7103 GLoss: 0.2502\n",
      "[5/10][30/391] DLoss: -1.1031 GLoss: 0.6340\n",
      "[5/10][35/391] DLoss: -1.1831 GLoss: 0.6125\n",
      "[5/10][40/391] DLoss: -1.1825 GLoss: 0.6453\n",
      "[5/10][45/391] DLoss: -1.1274 GLoss: 0.5531\n",
      "[5/10][50/391] DLoss: -1.0420 GLoss: 0.6511\n",
      "[5/10][55/391] DLoss: -1.1002 GLoss: 0.5496\n",
      "[5/10][60/391] DLoss: -0.9215 GLoss: 0.6388\n",
      "[5/10][65/391] DLoss: -0.9130 GLoss: 0.3100\n",
      "[5/10][70/391] DLoss: -0.8315 GLoss: 0.3480\n",
      "[5/10][75/391] DLoss: -1.0951 GLoss: 0.5495\n",
      "[5/10][80/391] DLoss: -1.1320 GLoss: 0.6219\n",
      "[5/10][85/391] DLoss: -1.1340 GLoss: 0.5887\n",
      "[5/10][90/391] DLoss: -1.0101 GLoss: 0.6542\n",
      "[5/10][95/391] DLoss: -0.7490 GLoss: 0.3025\n"
     ]
    }
   ],
   "source": [
    "# Training algorithm for discriminator and generator\n",
    "for epoch in range(num_epochs):\n",
    "    epochData = iter(data_loader)\n",
    "    data_counter = 0\n",
    "    \n",
    "    # Iterate for all batches of data\n",
    "    while data_counter < len(data_loader):\n",
    "        for param in discriminator.parameters():\n",
    "            param.requires_grad=True\n",
    "        \n",
    "        # Iterate until critic requirement is satisfied\n",
    "        critic_counter = 0\n",
    "        while data_counter < len(data_loader) and critic_counter < critic_iter:\n",
    "            data = epochData.next()\n",
    "            critic_counter += 1\n",
    "            for param in discriminator.parameters():\n",
    "                param.data.clamp_(-1e-2, 1e-2)\n",
    "            data_counter += 1\n",
    "            \n",
    "            # Train Discriminator with real data\n",
    "            dis_optim.zero_grad()\n",
    "            real_data, _ = data\n",
    "            size_of_batch = real_data.size(0)\n",
    "            real_data = real_data\n",
    "            input_tensor.resize_as_(real_data).copy_(real_data)\n",
    "            input_var = Variable(input_tensor)\n",
    "            dis_real_error = discriminator(input_var)\n",
    "            dis_real_error.backward(ones_tensor)\n",
    "\n",
    "            # Train Discriminator on fake data\n",
    "            noise.resize_(size_of_batch, noise_dim, 1,1).normal_(0,1)\n",
    "            noise_var = Variable(noise)\n",
    "            fake_data = generator(noise_var)\n",
    "            dis_fake_error = discriminator(fake_data.detach())\n",
    "            dis_fake_error.backward(nOnesTensor)\n",
    "            dis_optim.step()\n",
    "            final_dis_error = -dis_fake_error + dis_real_error\n",
    "            \n",
    "        # Train Generator\n",
    "        for param in discriminator.parameters():\n",
    "            param.requires_grad = False\n",
    "        gen_optim.zero_grad()\n",
    "        gen_error = discriminator(fake_data)\n",
    "        gen_error.backward(ones_tensor)\n",
    "        gen_optim.step()\n",
    "        \n",
    "        \n",
    "        print('[%d/%d][%d/%d] DLoss: %.4f GLoss: %.4f' % \n",
    "             (epoch, num_epochs, data_counter, len(data_loader), final_dis_error.data[0], gen_error.data[0]))\n",
    "\n",
    "        count += 1\n",
    "        count_list.append(count)\n",
    "        gen_loss_list.append(gen_error.data.cpu().numpy()[0])\n",
    "        disc_loss_list.append(final_dis_error.data.cpu().numpy()[0])\n",
    "    \n",
    "    # Store fake images\n",
    "    fake_data = generator(norm_noise)\n",
    "    fake_data.data = fake_data.data.mul(0.5).add(0.5)\n",
    "    vutils.save_image(fake_data.data, '%s/fake_samples_epoch_%03d.png' % \n",
    "                      (output_folder, epoch), normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f06f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss of the generator and the descriminator\n",
    "# plot predictions for arcsinh(x) and compate to ground truth\n",
    "plt.plot(count_list, gen_loss_list, 'r.', label='Generator')\n",
    "plt.plot(count_list, disc_loss_list, 'b.', label='Discriminator')\n",
    "plt.title(\"WGAN Loss of Discriminator and Generator\")\n",
    "plt.xlabel(\"Batch Number\")\n",
    "plt.ylabel(\"Binary Cross Entropy Loss\")\n",
    "plt.legend(loc = \"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a923cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
